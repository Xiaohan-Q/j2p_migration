# æ™ºèƒ½è¯­ä¹‰ç†è§£è¿ç§»æŒ‡å—

## ğŸ¯ æ¦‚è¿°

ç°åœ¨ä½ çš„é¡¹ç›®å·²ç»æ”¯æŒ**çœŸæ­£çš„æ™ºèƒ½ä»£ç è¿ç§»**,å…·å¤‡ä»¥ä¸‹èƒ½åŠ›:

âœ… **è¯­ä¹‰ç†è§£** - ç†è§£ä»£ç çš„ä¸šåŠ¡å«ä¹‰å’Œè®¾è®¡æ„å›¾
âœ… **å®Œæ•´å®ç°** - è‡ªåŠ¨ç”Ÿæˆæ–¹æ³•ä½“é€»è¾‘,ä¸åªæ˜¯ç­¾å
âœ… **æ™ºèƒ½é‡æ„** - è‡ªåŠ¨è½¬æ¢ä¸º Pythonic é£æ ¼
âœ… **è®¾è®¡æ¨¡å¼è¯†åˆ«** - è¯†åˆ«å¹¶ä¿ç•™è®¾è®¡æ¨¡å¼
âœ… **æ··åˆæ¨¡å¼** - è‡ªåŠ¨é€‰æ‹©æœ€ä½³è¿ç§»ç­–ç•¥

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–(å·²æœ‰)
pip install -r requirements.txt

# LLM ä¾èµ–(é€‰æ‹©ä¸€ä¸ª)
pip install openai        # OpenAI GPT-4
pip install anthropic     # Anthropic Claude
pip install requests      # Ollama (æœ¬åœ°å…è´¹)
```

### 2. é…ç½® LLM

#### æ–¹æ¡ˆ A: OpenAI (æ¨è)
```bash
export OPENAI_API_KEY="sk-..."
```

#### æ–¹æ¡ˆ B: Anthropic Claude
```bash
export ANTHROPIC_API_KEY="sk-ant-..."
```

#### æ–¹æ¡ˆ C: Ollama (æœ¬åœ°å…è´¹)
```bash
# å®‰è£… Ollama
curl https://ollama.ai/install.sh | sh

# ä¸‹è½½æ¨¡å‹
ollama pull codellama

# å¯åŠ¨æœåŠ¡
ollama serve
```

### 3. è¿è¡Œæ¼”ç¤º

```bash
# è¿è¡Œå®Œæ•´æ¼”ç¤º(ä½¿ç”¨ Mock LLM,ä¸éœ€è¦ API key)
python demo_intelligent.py

# ä½¿ç”¨çœŸå® LLM çš„æ¼”ç¤º
# éœ€è¦å…ˆé…ç½® API key
python demo_intelligent.py
```

---

## ğŸ“š ä½¿ç”¨æ–¹æ³•

### Python API ä½¿ç”¨

#### åŸºç¡€ä½¿ç”¨

```python
from intelligent_migrator import IntelligentMigrator, MigrationMode
from llm_providers import create_llm_provider

# 1. åˆ›å»º LLM æä¾›è€…
provider = create_llm_provider("openai")  # æˆ– "anthropic", "ollama"

# 2. åˆ›å»ºæ™ºèƒ½è¿ç§»å™¨
migrator = IntelligentMigrator(
    llm_provider=provider,
    mode=MigrationMode.HYBRID  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ¨¡å¼
)

# 3. æ‰§è¡Œè¿ç§»
java_code = """
public class UserService {
    public User createUser(String email, String name) {
        if (!isValidEmail(email)) {
            throw new IllegalArgumentException("Invalid email");
        }
        return new User(email, name);
    }
}
"""

results = migrator.migrate(
    java_code,
    validate=True,      # éªŒè¯ç”Ÿæˆçš„ä»£ç 
    refactor=True       # é‡æ„ä¸º Pythonic é£æ ¼
)

# 4. æŸ¥çœ‹ç»“æœ
if results['success']:
    print(results['python_code'])
    print(f"æ¨¡å¼: {results['mode_used']}")
    print(f"ä¸šåŠ¡åˆ†æ: {results['business_analysis']}")
else:
    print(f"é”™è¯¯: {results['errors']}")
```

#### ä¸‰ç§æ¨¡å¼è¯¦è§£

```python
# æ¨¡å¼ 1: è§„åˆ™æ˜ å°„ (å¿«é€Ÿã€å…è´¹)
migrator = IntelligentMigrator(mode=MigrationMode.RULE_BASED)
# é€‚ç”¨: ç®€å• POJOã€DTOã€å®ä½“ç±»

# æ¨¡å¼ 2: è¯­ä¹‰ç†è§£ (é«˜è´¨é‡ã€éœ€è¦ LLM)
migrator = IntelligentMigrator(
    llm_provider=provider,
    mode=MigrationMode.SEMANTIC
)
# é€‚ç”¨: å¤æ‚ä¸šåŠ¡é€»è¾‘ã€æœåŠ¡ç±»ã€ç®—æ³•

# æ¨¡å¼ 3: æ··åˆæ¨¡å¼ (æ¨è)
migrator = IntelligentMigrator(
    llm_provider=provider,
    mode=MigrationMode.HYBRID  # è‡ªåŠ¨é€‰æ‹©
)
# è‡ªåŠ¨å†³ç­–: ç®€å•ç±»ç”¨è§„åˆ™,å¤æ‚ç±»ç”¨ LLM
```

---

## ğŸ”§ LLM æä¾›è€…é…ç½®

### OpenAI (æœ€æ¨è)

**ä¼˜ç‚¹**: è´¨é‡æœ€é«˜ã€é€Ÿåº¦å¿«ã€ç¨³å®š
**ç¼ºç‚¹**: éœ€è¦ä»˜è´¹

```python
from llm_providers import OpenAIProvider

provider = OpenAIProvider(
    api_key="sk-...",                    # å¯é€‰,é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
    model="gpt-4-turbo-preview",         # æˆ– "gpt-3.5-turbo" æ›´ä¾¿å®œ
    base_url="https://api.openai.com/v1" # å¯é€‰,ç”¨äºä»£ç†
)
```

**æˆæœ¬ä¼°ç®—**:
- GPT-4: ~$0.01-0.03/æ¬¡
- GPT-3.5: ~$0.001-0.002/æ¬¡

### Anthropic Claude

**ä¼˜ç‚¹**: è´¨é‡é«˜ã€ä¸Šä¸‹æ–‡é•¿
**ç¼ºç‚¹**: éœ€è¦ä»˜è´¹

```python
from llm_providers import AnthropicProvider

provider = AnthropicProvider(
    api_key="sk-ant-...",                      # å¯é€‰
    model="claude-3-5-sonnet-20241022"         # æˆ– "claude-3-opus"
)
```

**æˆæœ¬ä¼°ç®—**:
- Claude 3.5 Sonnet: ~$0.01-0.02/æ¬¡

### Ollama (å…è´¹æ¨è)

**ä¼˜ç‚¹**: å®Œå…¨å…è´¹ã€æ•°æ®éšç§
**ç¼ºç‚¹**: éœ€è¦æœ¬åœ° GPUã€è´¨é‡ç•¥ä½

```python
from llm_providers import OllamaProvider

provider = OllamaProvider(
    model="codellama",                    # æˆ– "llama2", "mistral"
    base_url="http://localhost:11434"    # Ollama æœåŠ¡åœ°å€
)
```

**æ¨¡å‹æ¨è**:
- `codellama`: ä»£ç ä¸“ç”¨,æœ€æ¨è
- `llama2`: é€šç”¨æ¨¡å‹
- `mistral`: è½»é‡é«˜æ•ˆ

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ ¹æ®é¡¹ç›®è§„æ¨¡é€‰æ‹©æ¨¡å¼

```python
# å°é¡¹ç›® (<100ä¸ªç±») - å…¨éƒ¨ä½¿ç”¨è§„åˆ™æ˜ å°„
migrator = IntelligentMigrator(mode=MigrationMode.RULE_BASED)

# ä¸­å‹é¡¹ç›® (100-1000ä¸ªç±») - æ··åˆæ¨¡å¼
migrator = IntelligentMigrator(
    llm_provider=provider,
    mode=MigrationMode.HYBRID
)

# å¤§å‹é¡¹ç›® (>1000ä¸ªç±») - åˆ†æ‰¹å¤„ç†
# ç®€å•ç±»ç”¨è§„åˆ™,æ ¸å¿ƒç±»ç”¨è¯­ä¹‰
```

### 2. æˆæœ¬ä¼˜åŒ–

```python
# ç­–ç•¥ 1: å…ˆç”¨å…è´¹æ¨¡å¼é¢„è§ˆ
mock_migrator = IntelligentMigrator(
    llm_provider=create_llm_provider("mock"),
    mode=MigrationMode.HYBRID
)
results = mock_migrator.migrate(java_code)

# ç­–ç•¥ 2: åªå¯¹å¤æ‚ç±»ä½¿ç”¨ LLM
if results['mode_used'] == MigrationMode.SEMANTIC:
    # ä½¿ç”¨çœŸå® LLM é‡æ–°ç”Ÿæˆ
    real_migrator = IntelligentMigrator(
        llm_provider=create_llm_provider("openai"),
        mode=MigrationMode.SEMANTIC
    )
    results = real_migrator.migrate(java_code)
```

### 3. æ‰¹é‡è¿ç§»è„šæœ¬

```python
import os
from pathlib import Path

def migrate_project(java_dir: str, output_dir: str):
    """æ‰¹é‡è¿ç§»æ•´ä¸ªé¡¹ç›®"""

    migrator = IntelligentMigrator(
        llm_provider=create_llm_provider("openai"),
        mode=MigrationMode.HYBRID
    )

    for java_file in Path(java_dir).rglob("*.java"):
        with open(java_file, 'r', encoding='utf-8') as f:
            java_code = f.read()

        results = migrator.migrate(java_code)

        if results['success']:
            # ä¿å­˜ Python æ–‡ä»¶
            rel_path = java_file.relative_to(java_dir)
            py_file = Path(output_dir) / rel_path.with_suffix('.py')
            py_file.parent.mkdir(parents=True, exist_ok=True)

            with open(py_file, 'w', encoding='utf-8') as f:
                f.write(results['python_code'])

            print(f"âœ“ {java_file} -> {py_file}")
        else:
            print(f"âœ— {java_file} å¤±è´¥: {results['errors']}")

# ä½¿ç”¨
migrate_project("./java_project/src", "./python_project/src")
```

---

## ğŸ“Š æ•ˆæœå¯¹æ¯”

### ç¤ºä¾‹: UserService è¿ç§»

#### è¾“å…¥ (Java)
```java
public class UserService {
    private UserRepository repository;

    public User createUser(String email, String name) {
        if (!isValidEmail(email)) {
            throw new IllegalArgumentException("Invalid email");
        }

        User user = new User(email, name);
        user.setCreatedAt(new Date());

        return repository.save(user);
    }

    private boolean isValidEmail(String email) {
        return email != null && email.contains("@");
    }
}
```

#### è¾“å‡ºå¯¹æ¯”

**è§„åˆ™æ˜ å°„æ¨¡å¼** (å¿«é€Ÿä½†ä¸å®Œæ•´):
```python
class UserService:
    """Java ç±» UserService çš„ Python å®ç°"""

    def create_user(self, email: str, name: str) -> User:
        """TODO: å®ç°æ–¹æ³•ä½“"""
        pass  # âŒ éœ€è¦æ‰‹åŠ¨å®ç°

    def _is_valid_email(self, email: str) -> bool:
        """TODO: å®ç°æ–¹æ³•ä½“"""
        pass  # âŒ éœ€è¦æ‰‹åŠ¨å®ç°
```

**è¯­ä¹‰ç†è§£æ¨¡å¼** (å®Œæ•´å®ç°):
```python
from datetime import datetime
from typing import Optional

class UserService:
    """
    ç”¨æˆ·æœåŠ¡ç±» - å¤„ç†ç”¨æˆ·åˆ›å»ºå’ŒéªŒè¯

    èŒè´£:
    - åˆ›å»ºæ–°ç”¨æˆ·å¹¶æŒä¹…åŒ–
    - éªŒè¯ç”¨æˆ·é‚®ç®±æ ¼å¼
    """

    def __init__(self, repository: 'UserRepository'):
        self._repository = repository

    def create_user(self, email: str, name: str) -> 'User':
        """
        åˆ›å»ºæ–°ç”¨æˆ·

        Args:
            email: ç”¨æˆ·é‚®ç®±
            name: ç”¨æˆ·åç§°

        Returns:
            åˆ›å»ºçš„ç”¨æˆ·å¯¹è±¡

        Raises:
            ValueError: é‚®ç®±æ ¼å¼æ— æ•ˆ
        """
        if not self._is_valid_email(email):
            raise ValueError("Invalid email")

        user = User(email=email, name=name)
        user.created_at = datetime.now()

        return self._repository.save(user)

    def _is_valid_email(self, email: str) -> bool:
        """éªŒè¯é‚®ç®±æ ¼å¼"""
        return email is not None and '@' in email
```

---

## ğŸ› æ•…éšœæ’é™¤

### é—®é¢˜ 1: LLM API è°ƒç”¨å¤±è´¥

```python
# é”™è¯¯: "OpenAI API è°ƒç”¨å¤±è´¥"
# è§£å†³:
# 1. æ£€æŸ¥ API key
import os
print(os.getenv('OPENAI_API_KEY'))

# 2. æ£€æŸ¥ç½‘ç»œ
# 3. å°è¯•ä½¿ç”¨ä»£ç†
provider = OpenAIProvider(
    api_key="sk-...",
    base_url="https://your-proxy.com/v1"
)
```

### é—®é¢˜ 2: Ollama è¿æ¥å¤±è´¥

```bash
# é”™è¯¯: "Ollama API è°ƒç”¨å¤±è´¥"
# è§£å†³:
# 1. ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ
ollama serve

# 2. æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
ollama list

# 3. å¦‚æœæ²¡æœ‰,ä¸‹è½½æ¨¡å‹
ollama pull codellama
```

### é—®é¢˜ 3: ç”Ÿæˆè´¨é‡ä¸ä½³

```python
# ç­–ç•¥ 1: æé«˜æ¸©åº¦å‚æ•°
results = migrator.migrate(java_code)  # ä½¿ç”¨é»˜è®¤ temperature=0.2

# ç­–ç•¥ 2: ä½¿ç”¨æ›´å¥½çš„æ¨¡å‹
provider = OpenAIProvider(model="gpt-4-turbo-preview")  # è€Œä¸æ˜¯ gpt-3.5

# ç­–ç•¥ 3: å¯ç”¨é‡æ„
results = migrator.migrate(java_code, refactor=True)
```

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. ç¼“å­˜ LLM å“åº”

```python
import hashlib
import json

class CachedLLMProvider:
    def __init__(self, base_provider, cache_file="llm_cache.json"):
        self.provider = base_provider
        self.cache_file = cache_file
        self.cache = self._load_cache()

    def complete(self, prompt, system=None, **kwargs):
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = hashlib.md5(
            (prompt + (system or "")).encode()
        ).hexdigest()

        # æ£€æŸ¥ç¼“å­˜
        if cache_key in self.cache:
            return self.cache[cache_key]

        # è°ƒç”¨ LLM
        response = self.provider.complete(prompt, system, **kwargs)

        # ä¿å­˜ç¼“å­˜
        self.cache[cache_key] = response
        self._save_cache()

        return response

    def _load_cache(self):
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except:
            return {}

    def _save_cache(self):
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
```

### 2. å¹¶è¡Œå¤„ç†

```python
from concurrent.futures import ThreadPoolExecutor

def migrate_files_parallel(java_files, max_workers=5):
    """å¹¶è¡Œè¿ç§»å¤šä¸ªæ–‡ä»¶"""
    migrator = IntelligentMigrator(...)

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for java_file in java_files:
            future = executor.submit(migrate_single_file, java_file, migrator)
            futures.append(future)

        for future in futures:
            result = future.result()
            print(f"å®Œæˆ: {result}")
```

---

## ğŸ“ ä¸‹ä¸€æ­¥

ç°åœ¨ä½ å·²ç»æœ‰äº†å®Œæ•´çš„æ™ºèƒ½è¿ç§»ç³»ç»Ÿ!

**æ¨èæ“ä½œ:**
1. âœ… è¿è¡Œ `python demo_intelligent.py` æŸ¥çœ‹æ•ˆæœ
2. âœ… é…ç½®çœŸå®çš„ LLM (OpenAI/Anthropic/Ollama)
3. âœ… åœ¨å®é™…é¡¹ç›®ä¸­æµ‹è¯•
4. âœ… æ ¹æ®éœ€è¦è°ƒæ•´å¤æ‚åº¦é˜ˆå€¼

**å¯é€‰æ‰©å±•:**
- æ·»åŠ æ›´å¤šè®¾è®¡æ¨¡å¼è¯†åˆ«
- é›†æˆä»£ç è´¨é‡æ£€æŸ¥
- æ·»åŠ  Web UI
- æ”¯æŒå¢é‡è¿ç§»

---

## ğŸ’¬ åé¦ˆå’Œæ”¯æŒ

é‡åˆ°é—®é¢˜? æœ‰å»ºè®®?
- æŸ¥çœ‹ [AGENT_COMPARISON.md](AGENT_COMPARISON.md) äº†è§£è¯¦ç»†å¯¹æ¯”
- æŸ¥çœ‹æºä»£ç æ³¨é‡Šäº†è§£å®ç°ç»†èŠ‚
- æäº¤ issue æˆ– PR

**äº«å—æ™ºèƒ½ä»£ç è¿ç§»! ğŸ‰**
